#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de validation compl√®te du syst√®me SAMA CONAI
Teste toutes les fonctionnalit√©s et g√©n√®re un rapport de conformit√©
"""

def validate_complete_system():
    """
    Validation compl√®te du syst√®me SAMA CONAI
    """
    print("üîç VALIDATION COMPL√àTE DU SYST√àME SAMA CONAI")
    print("=" * 60)
    
    validation_results = {
        'models': {},
        'views': {},
        'data': {},
        'workflows': {},
        'security': {},
        'performance': {}
    }
    
    try:
        # ========================================
        # 1. VALIDATION DES MOD√àLES
        # ========================================
        print("üìã 1. VALIDATION DES MOD√àLES")
        print("-" * 30)
        
        models_to_test = [
            'request.information',
            'request.information.stage',
            'request.refusal.reason',
            'whistleblowing.alert',
            'whistleblowing.alert.stage'
        ]
        
        for model_name in models_to_test:
            try:
                model = env[model_name]
                count = model.search_count([])
                validation_results['models'][model_name] = {
                    'exists': True,
                    'count': count,
                    'status': 'OK'
                }
                print(f"   ‚úÖ {model_name}: {count} enregistrements")
            except Exception as e:
                validation_results['models'][model_name] = {
                    'exists': False,
                    'error': str(e),
                    'status': 'ERROR'
                }
                print(f"   ‚ùå {model_name}: ERREUR - {e}")
        
        print()
        
        # ========================================
        # 2. VALIDATION DES VUES
        # ========================================
        print("üé® 2. VALIDATION DES VUES")
        print("-" * 30)
        
        view_types = ['kanban', 'list', 'form', 'graph', 'pivot', 'search']
        main_models = ['request.information', 'whistleblowing.alert']
        
        for model in main_models:
            validation_results['views'][model] = {}
            for view_type in view_types:
                try:
                    views = env['ir.ui.view'].search([
                        ('model', '=', model),
                        ('type', '=', view_type)
                    ])
                    if views:
                        validation_results['views'][model][view_type] = {
                            'exists': True,
                            'count': len(views),
                            'status': 'OK'
                        }
                        print(f"   ‚úÖ {model} - {view_type}: {len(views)} vue(s)")
                    else:
                        validation_results['views'][model][view_type] = {
                            'exists': False,
                            'status': 'MISSING'
                        }
                        print(f"   ‚ö†Ô∏è  {model} - {view_type}: MANQUANT")
                except Exception as e:
                    validation_results['views'][model][view_type] = {
                        'exists': False,
                        'error': str(e),
                        'status': 'ERROR'
                    }
                    print(f"   ‚ùå {model} - {view_type}: ERREUR")
        
        print()
        
        # ========================================
        # 3. VALIDATION DES DONN√âES
        # ========================================
        print("üìä 3. VALIDATION DES DONN√âES")
        print("-" * 30)
        
        # Test de la richesse des donn√©es
        info_requests = env['request.information'].search([])
        wb_alerts = env['whistleblowing.alert'].search([])
        
        # Diversit√© des √©tats
        states = set(info_requests.mapped('state'))
        categories = set(wb_alerts.mapped('category'))
        priorities = set(wb_alerts.mapped('priority'))
        
        validation_results['data'] = {
            'info_requests_count': len(info_requests),
            'wb_alerts_count': len(wb_alerts),
            'states_diversity': len(states),
            'categories_diversity': len(categories),
            'priorities_diversity': len(priorities),
            'rich_content': {
                'requests_with_response': len([r for r in info_requests if r.response_body]),
                'refused_requests': len([r for r in info_requests if r.is_refusal]),
                'alerts_with_investigation': len([a for a in wb_alerts if a.investigation_notes]),
                'resolved_alerts': len([a for a in wb_alerts if a.resolution])
            }
        }
        
        print(f"   üìà Demandes d'information: {len(info_requests)}")
        print(f"   üö® Signalements d'alerte: {len(wb_alerts)}")
        print(f"   üéØ √âtats diff√©rents: {len(states)}")
        print(f"   üìÇ Cat√©gories diff√©rentes: {len(categories)}")
        print(f"   ‚ö° Priorit√©s diff√©rentes: {len(priorities)}")
        print(f"   üí¨ Demandes avec r√©ponse: {validation_results['data']['rich_content']['requests_with_response']}")
        print(f"   ‚úÖ Signalements r√©solus: {validation_results['data']['rich_content']['resolved_alerts']}")
        
        print()
        
        # ========================================
        # 4. VALIDATION DES WORKFLOWS
        # ========================================
        print("‚öôÔ∏è  4. VALIDATION DES WORKFLOWS")
        print("-" * 30)
        
        # Test des m√©thodes de workflow
        workflow_tests = {}
        
        # Test sur une demande d'information
        if info_requests:
            test_request = info_requests[0]
            try:
                # Test des m√©thodes sans les ex√©cuter r√©ellement
                methods_to_test = [
                    'action_submit',
                    'action_start_processing',
                    'action_request_validation',
                    '_compute_deadline_date',
                    '_compute_is_overdue'
                ]
                
                for method in methods_to_test:
                    if hasattr(test_request, method):
                        workflow_tests[f'info_request_{method}'] = 'OK'
                        print(f"   ‚úÖ Demande info - {method}: Disponible")
                    else:
                        workflow_tests[f'info_request_{method}'] = 'MISSING'
                        print(f"   ‚ùå Demande info - {method}: MANQUANT")
                        
            except Exception as e:
                print(f"   ‚ùå Erreur test workflow demandes: {e}")
        
        # Test sur un signalement d'alerte
        if wb_alerts:
            test_alert = wb_alerts[0]
            try:
                methods_to_test = [
                    'action_start_assessment',
                    'action_start_investigation',
                    'action_resolve',
                    '_generate_access_token'
                ]
                
                for method in methods_to_test:
                    if hasattr(test_alert, method):
                        workflow_tests[f'wb_alert_{method}'] = 'OK'
                        print(f"   ‚úÖ Signalement - {method}: Disponible")
                    else:
                        workflow_tests[f'wb_alert_{method}'] = 'MISSING'
                        print(f"   ‚ùå Signalement - {method}: MANQUANT")
                        
            except Exception as e:
                print(f"   ‚ùå Erreur test workflow signalements: {e}")
        
        validation_results['workflows'] = workflow_tests
        print()
        
        # ========================================
        # 5. VALIDATION DE LA S√âCURIT√â
        # ========================================
        print("üîí 5. VALIDATION DE LA S√âCURIT√â")
        print("-" * 30)
        
        # Test des groupes de s√©curit√©
        security_groups = [
            'sama_conai.group_info_request_user',
            'sama_conai.group_info_request_manager',
            'sama_conai.group_whistleblowing_manager',
            'sama_conai.group_transparency_admin'
        ]
        
        security_results = {}
        for group_ref in security_groups:
            try:
                group = env.ref(group_ref)
                security_results[group_ref] = {
                    'exists': True,
                    'users_count': len(group.users),
                    'status': 'OK'
                }
                print(f"   ‚úÖ {group.name}: {len(group.users)} utilisateur(s)")
            except Exception as e:
                security_results[group_ref] = {
                    'exists': False,
                    'error': str(e),
                    'status': 'ERROR'
                }
                print(f"   ‚ùå {group_ref}: ERREUR")
        
        # Test des r√®gles d'acc√®s
        access_rules = env['ir.model.access'].search([
            ('model_id.model', 'in', ['request.information', 'whistleblowing.alert'])
        ])
        
        security_results['access_rules'] = len(access_rules)
        print(f"   üõ°Ô∏è  R√®gles d'acc√®s: {len(access_rules)}")
        
        validation_results['security'] = security_results
        print()
        
        # ========================================
        # 6. VALIDATION DES PERFORMANCES
        # ========================================
        print("‚ö° 6. VALIDATION DES PERFORMANCES")
        print("-" * 30)
        
        import time
        
        # Test de performance des recherches
        start_time = time.time()
        env['request.information'].search([])
        search_time = time.time() - start_time
        
        # Test de performance des vues
        start_time = time.time()
        env['ir.ui.view'].search([('model', '=', 'request.information')])
        view_time = time.time() - start_time
        
        validation_results['performance'] = {
            'search_time': search_time,
            'view_time': view_time,
            'total_records': len(info_requests) + len(wb_alerts)
        }
        
        print(f"   üîç Temps de recherche: {search_time:.3f}s")
        print(f"   üé® Temps de chargement vues: {view_time:.3f}s")
        print(f"   üìä Total enregistrements: {validation_results['performance']['total_records']}")
        
        print()
        
        # ========================================
        # 7. RAPPORT DE VALIDATION
        # ========================================
        print("üìã 7. RAPPORT DE VALIDATION")
        print("-" * 30)
        
        # Calcul du score global
        total_tests = 0
        passed_tests = 0
        
        # Score mod√®les
        for model, result in validation_results['models'].items():
            total_tests += 1
            if result['status'] == 'OK':
                passed_tests += 1
        
        # Score vues
        for model, views in validation_results['views'].items():
            for view_type, result in views.items():
                total_tests += 1
                if result['status'] == 'OK':
                    passed_tests += 1
        
        # Score workflows
        for workflow, status in validation_results['workflows'].items():
            total_tests += 1
            if status == 'OK':
                passed_tests += 1
        
        # Score s√©curit√©
        for group, result in validation_results['security'].items():
            if isinstance(result, dict):
                total_tests += 1
                if result['status'] == 'OK':
                    passed_tests += 1
        
        # Score donn√©es (crit√®res de qualit√©)
        data_criteria = [
            validation_results['data']['info_requests_count'] >= 4,
            validation_results['data']['wb_alerts_count'] >= 4,
            validation_results['data']['states_diversity'] >= 3,
            validation_results['data']['categories_diversity'] >= 3,
            validation_results['data']['rich_content']['requests_with_response'] >= 1,
            validation_results['data']['rich_content']['resolved_alerts'] >= 1
        ]
        
        for criterion in data_criteria:
            total_tests += 1
            if criterion:
                passed_tests += 1
        
        # Score performance
        performance_criteria = [
            validation_results['performance']['search_time'] < 1.0,
            validation_results['performance']['view_time'] < 0.5
        ]
        
        for criterion in performance_criteria:
            total_tests += 1
            if criterion:
                passed_tests += 1
        
        # Calcul du score final
        success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        
        print(f"   üìä Tests r√©ussis: {passed_tests}/{total_tests}")
        print(f"   üéØ Taux de r√©ussite: {success_rate:.1f}%")
        print()
        
        # √âvaluation finale
        if success_rate >= 95:
            status = "üèÜ EXCELLENT"
            message = "Syst√®me pr√™t pour la production"
            color = "green"
        elif success_rate >= 85:
            status = "‚úÖ TR√àS BIEN"
            message = "Syst√®me pr√™t avec quelques am√©liorations mineures"
            color = "lightgreen"
        elif success_rate >= 75:
            status = "üëç BIEN"
            message = "Syst√®me fonctionnel, am√©liorations recommand√©es"
            color = "yellow"
        elif success_rate >= 60:
            status = "‚ö†Ô∏è  MOYEN"
            message = "Syst√®me partiellement fonctionnel, corrections n√©cessaires"
            color = "orange"
        else:
            status = "‚ùå INSUFFISANT"
            message = "Syst√®me non pr√™t, r√©vision majeure requise"
            color = "red"
        
        print(f"üéØ √âVALUATION FINALE: {status}")
        print(f"   {message}")
        print()
        
        # Recommandations
        print("üí° RECOMMANDATIONS:")
        
        if success_rate >= 85:
            print("   ‚úÖ Proc√©der √† la formation des utilisateurs")
            print("   ‚úÖ Configurer les notifications email")
            print("   ‚úÖ Personnaliser les workflows si n√©cessaire")
            print("   ‚úÖ Planifier la mise en production")
        else:
            print("   üîß Corriger les erreurs identifi√©es")
            print("   üìä Enrichir les donn√©es de d√©monstration")
            print("   üé® V√©rifier toutes les vues")
            print("   üîí Valider la configuration de s√©curit√©")
        
        print()
        print("üìö RESSOURCES DISPONIBLES:")
        print("   üìñ Guide d'analyse: GUIDE_ANALYSE_DONNEES.md")
        print("   üß™ Scripts de test: scripts/")
        print("   üìã Documentation: README.md")
        print("   üîß Scripts d'installation: scripts/install_complete_demo.sh")
        
        return success_rate >= 75
        
    except Exception as e:
        print(f"‚ùå Erreur critique lors de la validation: {e}")
        import traceback
        traceback.print_exc()
        return False

# Ex√©cuter la validation si dans le contexte Odoo
try:
    result = validate_complete_system()
    print("\\n" + "=" * 60)
    if result:
        print("üéâ SAMA CONAI: VALIDATION R√âUSSIE!")
        print("   Le syst√®me est pr√™t pour l'utilisation")
    else:
        print("üîß SAMA CONAI: VALIDATION PARTIELLE")
        print("   Des am√©liorations sont n√©cessaires")
    print("=" * 60)
except NameError:
    print("‚ùå Ce script doit √™tre ex√©cut√© dans le shell Odoo.")
    print("   Usage: ./odoo-bin shell -c odoo.conf -d your_database")
    print("   Puis: exec(open('scripts/validate_complete_system.py').read())")